# Open container registry and remind the audience of the automated build that ran when you checked in some code.  Show them the name - it is not tagged with a version, like the one you did by hand, but uses a hash. We don't want that for CI/CD, so it's time to change the trigger.

# Return to cloud build and edit the  trigger (via the vertial . . . on the right)

# Change to a tag based build trigger, and modify the image name to use $TAG_NAME

gcr.io/kr-dr-temp-hip/ui:$TAG_NAME

# Now that you've updated the trigger, it's time to make a change and commit.

# Use the code editor to edit main.py in the frontend folder. Locate the code setting model['greeting'] (around line 51) and change it as follows, then save the file.

 model['greeting'] = 'Hello from the [location] onboard'

# Having made a change, it's time to commit the change and push it to the hiplocal repo

git add -A
git commit -m "Change to greeting"
git push hiplocal master

# show the trigger history - nothing is happening. That's because we changed the build to only happen when a commit is tagged.  Create a tag with the same format as the existing one.

git tag v0.2

# Next, push it to the repo - and then go back to build history. This time, it should build.

git push hiplocal --tags


# Open the frontend yaml file and modify the container image to use v0.2

        image: gcr.io/kr-dr-temp-hip/ui:v0.2

# Open a browser to the site, showing the current greeting. Keep it open - you will come back to it during the rolling update.

# Navigate to the frontend folder in cloud shell and apply the change. 

kubectl apply -f kubernetes-config.yaml

# G.get the pods and watch them turnover: the slow start on our readiness probe 
# is really helpful here....  Point out the number of pods at any one time.  
# When at least one pod has changed over, refresh the site repeatedly until 
# you see the new greeting.

kubectl get pods

# That worked great. Unfortunately, it seems some users have been having trouble 
# accessing the site, and this happened around the time of the change. It's doubtful 
# our change had anything to do with the problem, but she wants to roll bak just 
# in case. Luckily, that's really easy!

# Return to the yaml, revert back to v0.1 and then run kubectl apply again. 
# Watch the process via pods and browser, just as you did before.


#DONE